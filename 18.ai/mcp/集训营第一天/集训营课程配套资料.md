# 集训营

>20:30 准时开始

前端要被 AI 取代了 ？

前端和 AI 是共生的关系

1. AI 公司，会给前端带来新的岗位，普通用户和 AI 交互的主流方式就是 Web 应用
2. AI 是给前端赋能的
   - AI 相关的经验会成为你简历的难点亮点



```
大模型 --> Agent --> Function Calling --> MCP
```

面向零基础学员

>**安全依赖审计工具（Security-Check）｜MCP Server / Node.js / LLM 工具链**
>
>构建基于 MCP 协议的依赖漏洞审计服务，支持解析项目依赖项并自动检测 CVE、CVSS 等安全风险，返回结构化审计报告。工具同时适配本地开发环境与云端智能体平台，可作为 LLM 工具链中的安全能力模块灵活复用。
>
>该服务基于 npm 官方 Registry 审计 API，支持解析 `dependencies` 与 `devDependencies` 字段，映射 GitHub Advisory、CVSS 等漏洞源。支持多种调用方式：传入依赖列表、解析完整 `package.json`、通过路径读取依赖，**可与文件系统类工具组合形成自动审计链路**。
>
>工具已集成至 Cursor 编辑器，作为 AI 编码工具链中的核心安全插件；也可作为 LangChain.js、Dify 等平台下智能体统一调度的工具模块接入。
>
>单次调用延迟低于 200ms，漏洞识别准确率达 95%+，审计结果结构化，可供智能体进一步推理处理。项目落地后已在 20+ 内部项目中实际使用，辅助识别依赖漏洞近 50 项，**人工介入时间从约 20 分钟缩短至 2 分钟，效率提升达 90%+**。



## 大模型

大模型，全称“大语言模型”，英语 Large Language Model，简称 LLM.

🙋 LLM 是否等价于 AI ？

> 回答：不等于
>
> AI：所有模仿人类行为的算法、模型、系统都属于 AI 的范畴
>
> - 机器人
> - 计算机视觉
> - 自动驾驶
> - 语音识别
> - 自然语言处理（NLP 里面的一种实现方式）
> - ....

前期将 LLM 看作是一个黑盒。🤔如何和这个黑盒进行交互？

**输入** --> **输出**

<img src="https://xiejie-typora.oss-cn-chengdu.aliyuncs.com/2025-07-04-231458.png" alt="image-20250705071457641" style="zoom:50%;" />

大语言模型有很多：

1. Llama（Meta）
2. qwen（阿里的千问）
3. deepseek（深度求索）
4. .....

基于Transformer架构

我们需要知道大语言模型工作的本质就是 **输入** 和 **输出**。



**大模型的使用**

1. 官方提供的应用
   - ChatGPT：Web应用、GPT 是模型
   - 功能固定
2. 远程 API 调用大模型
   - 算力强
   - 收费
   - 隐私安全问题
3. 本地部署：只能部署开源的模型
   - Ollama（Meta）

本次集训营选择远程 API 调用大模型的形式，模型选择 deepseek.

[deepseek 开发平台](https://platform.deepseek.com/) 申请一个 api key

创建 api key 的时候要保存下来，不然之后就看不到了



用 `curl` 命令和大模型进行交互：

```bash
curl https://api.deepseek.com/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer sk-73d4caa5bc8149fa8ef17c76463a1003" \
  -d '{
        "model": "deepseek-chat",
        "messages": [
          {"role": "user", "content": "你是谁？"}
        ],
        "stream": true
      }'
```

返回的内容如下：

```json
{
  "id": "81dddbb5-c01b-476b-a5d5-c5ac87e31b92", // 每次请求的唯一标识符（UUID）
  "object": "chat.completion",                 // 返回对象类型，这里表示是聊天补全结果
  "created": 1751766353,                       // 响应生成时间（Unix 时间戳，单位秒）
  "model": "deepseek-chat",                    // 使用的模型名称

  "choices": [                                 // 模型返回的一个或多个回答（通常只会有一个）
    {
      "index": 0,                              // 当前回答的索引（从 0 开始）
      "message": {
        "role": "assistant",                   // 消息角色，这里是 AI 助手
        "content": "我是一个智能AI助手，随时为你提供帮助和解答问题。有什么我可以帮你的吗？" // 模型生成的文本内容
      },
      "logprobs": null,                        // token 概率信息（如未启用 logprobs 则为 null）
      "finish_reason": "stop"                  // 结束原因：stop 表示正常生成结束
    }
  ],

  "usage": {                                   // 本次请求的 Token 使用情况
    "prompt_tokens": 25,                       // 用户输入占用的 token 数
    "completion_tokens": 19,                   // 模型回复占用的 token 数
    "total_tokens": 44,                        // 总共使用的 token 数
    "prompt_tokens_details": {
      "cached_tokens": 0                       // 命中缓存的 token 数（如启用 prompt 缓存机制）
    },
    "prompt_cache_hit_tokens": 0,              // 缓存命中 token 数（本次为 0，表示未命中）
    "prompt_cache_miss_tokens": 25             // 缓存未命中 token 数（本次为 25，即全部未命中）
  },

  "system_fingerprint": "fp_8802369eaa_prod0623_fp8_kvcache" // 系统/模型运行环境指纹，用于追踪模型版本等
}
```

目前的大模型仅仅是简单的 **输入** 和 **输出**，甚至连上下文都不支持。

```bash
curl https://api.deepseek.com/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer sk-73d4caa5bc8149fa8ef17c76463a1003" \
  -d '{
        "model": "deepseek-chat",
        "messages": [
          {"role": "user", "content": "你是谁？"}
        ],
        "stream": false
      }'
```

```bash
curl https://api.deepseek.com/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer sk-73d4caa5bc8149fa8ef17c76463a1003" \
  -d '{
        "model": "deepseek-chat",
        "messages": [
          {"role": "user", "content": "我刚才说的啥？"}
        ],
        "stream": false
      }'
```



🙋那有没有办法让模型支持上下文呢？

只需要将之前的会话加入到 messages 数组里面就可以了。

```bash
curl https://api.deepseek.com/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer sk-73d4caa5bc8149fa8ef17c76463a1003" \
  -d '{
        "model": "deepseek-chat",
        "messages": [
          {"role": "user", "content": "你是谁？"},
          {"role": "system", "content": "我是一个智能AI助手，随时为你提供帮助和解答问题！有什么我可以帮你的吗？"},
          {"role": "user", "content": "我刚才说的啥？"}
        ],
        "stream": false
      }'
```

将之前的聊天记录一并给他。



🤔 假设我们现在要做一个 AI 聊天应用，还有哪些问题需要解决？

需要做成一个 Web 应用

不能对用户暴露 API key

还有一些其它的特性：获取实时信息

存储上下文



## Agent

这里介绍的 Agent，就是一个代理服务器，充当用户和模型交流的中间人。

<img src="https://xiejie-typora.oss-cn-chengdu.aliyuncs.com/2025-07-06-051934.png" alt="image-20250706131933681" style="zoom:50%;" />

任何服务器端语言都可以写

Node.js

**课堂演示**

课件《01. 聊天机器人》



有了中间层，能够更好的管理上下文。

**课堂演示**

课件《02. 带上下文的聊天机器人》



🤔 这里的 Agent 是不是智能体，或者说和智能体有什么区别？

Agent - 代理服务器，技术概念

Agent 智能体，是一个功能概念，相当于是给用户 **感官层面** 的一个事物



## Function Calling

🤔如何能够让模型获取实时信息呢？

可以采用这样的一种方案：

<img src="https://xiejie-typora.oss-cn-chengdu.aliyuncs.com/2025-07-07-072011.png" alt="image-20250707152011083" style="zoom:50%;" />



那这里有一个问题，工具箱以什么样的形式发送过去呢？

早期的时候，工具箱是以 **提示词** 的方式带过去的，这个很好理解，因为模型只能接收一段 **输入**，然后返回一段 **输出**。

**课堂演示**

课件《03. 聊天机器人获取实时信息》



🙋这种方式的问题？

1. 繁琐：这些大量的提示词，仅仅是为了约束大模型的输出
2. 不标准
3. 约束力不高



OpenAI 在 2023 年 6 月推出了 Function Calling，通过 JSON Schema 格式来进行标准化，主要标准两个部分：

1. 工具箱的提供

   ```js
   // 工具箱
   const tools = [{
       type: "function",
       name: "get_weather",
       description: "Get current temperature for provided coordinates in celsius.",
       parameters: {
           type: "object",
           properties: {
               latitude: { type: "number" },
               longitude: { type: "number" }
           },
           required: ["latitude", "longitude"],
           additionalProperties: false
       },
       strict: true
   }];
   ```
   
2. 返回的调用工具请求

   ```js
   [{
       "type": "function_call",
       "id": "fc_12345xyz",
       "call_id": "call_12345xyz",
       "name": "get_weather",
       "arguments": "{\"latitude\":48.8566,\"longitude\":2.3522}"
   }]
   ```

可以在 [OpenAI 官方文档](https://platform.openai.com/docs/guides/function-calling?api-mode=responses&lang=javascript#function-calling-steps) 看到这个过程，也可以在 [Playground](https://platform.openai.com/playground/prompts?models=gpt-4.1) 这里体验整个过程。

🤔 为什么采用 Function Calling 能约束大模型，让它要**一定**按照要求输出一个 JSON ？



**课堂演示**

课件《04. FunctionCalling版本聊天机器人》



Function Calling 的出现，解决了前面所说的三个问题：

1. 提示词繁琐
2. 提示词描述不标准
3. 模型输出不可控



假设现在要做一个查询天气的公共工具，提供给众多的 Agent 使用，如下图：

<img src="https://xiejie-typora.oss-cn-chengdu.aliyuncs.com/2025-07-08-063245.png" alt="image-20250708143245500" style="zoom:45%;" />

有一个非常棘手的问题，不同模型，注册到 Agent 里面的 Function Calling 的 JSON 格式是不一致的。

deepseek

```js
tools = [{
  "type": "function",
  "function": {
      "name": "get_weather",
      "description": "Get weather of an location",
      "parameters": {
          "type": "object",
          "properties": {
              "location": {
                  "type": "string",
                  "description": "The city and state, e.g. San Francisco, CA",
              }
          },
          "required": ["location"]
      },
  }
}]
```

GPT

```js
const tools = [{
    type: "function",
    name: "get_weather",
    description: "Get current temperature for provided coordinates in celsius.",
    parameters: {
        type: "object",
        properties: {
            latitude: { type: "number" },
            longitude: { type: "number" }
        },
        required: ["latitude", "longitude"],
        additionalProperties: false
    },
    strict: true
}];
```

claude

```js
"tools": [{
  "name": "get_weather",
  "description": "Get the current weather in a given location",
  "input_schema": {
    "type": "object",
    "properties": {
      "location": {
        "type": "string",
        "description": "The city and state, e.g. San Francisco, CA"
      }
    },
    "required": ["location"]
  }
}],
```

作为公共工具开发者，会遇到这样一个问题：

 <img src="https://xiejie-typora.oss-cn-chengdu.aliyuncs.com/2025-07-08-064123.png" alt="image-20250708144123440" style="zoom:50%;" />

**不同的模型，就意味着在 Agent 中注册工具的 JSON Schema 是不同的**。

🤔 如果是你，你会怎么办呢？

有些人可能会说，那我就开发多个适配不同版本的库，例如：

- 天气查询 deepseek 版
- 天气查询 gpt 版
- 天气查询 claude 版
- ......

<img src="https://xiejie-typora.oss-cn-chengdu.aliyuncs.com/2025-07-08-065326.png" alt="image-20250708145326895" style="zoom:50%;" />

这种方案，倒也不是说不行。

但是一旦未来出现了新的模型，意味着你这边就得开发一个适配新模型的天气查询工具。

久而久之，你自己也维护不过来。

🤔这一切的一切，是什么原因造成的？

标准



## MCP

MCP 全称为 Model Context Protocol，“模型上下文协议”。

但其实 **MCP 和模型间没什么关系**，而是描述 **Agent** 和 **外部工具** 的通信协议。

MCP 的本质，就是做了一层抽象。

给外部工具套上一层，这就是 MCP Server，在 Agent 内部也抽象出一层，这就是 MCP Client.

![image-20250708145513177](https://xiejie-typora.oss-cn-chengdu.aliyuncs.com/2025-07-08-065513.png)

有了这一层中间层之后，开发工具的人就舒服多了，只需要对接 MCP 标准格式去开发工具应用，根本不需要关心背后是什么大模型。

<img src="https://xiejie-typora.oss-cn-chengdu.aliyuncs.com/2025-07-07-213413.png" alt="image-20250708053412714" style="zoom:50%;" />

至于不同模型对应的不同 Function Calling 的 JSON 格式转换工作，交给 Agent 来处理。



1. JSON-RPC
2. 通信机制



### JSON-RPC

全称 JSON Remote Procedure Call，中文是“JSON 远程函数调用”。

**RPC（Remote Procedure Call，远程过程调用）** 是一种 **通信协议**，用于在不同的进程之间调用函数，就像调用本地函数一样。可以用于：

- 同一台机器上的不同进程（跨进程通信）
- 不同机器之间（跨网络通信）

RPC 实现有很多，比如：

- gRPC（Google 的高性能 RPC）
- JSON-RPC（比如 MCP 协议就基于这个）
- XML-RPC

MCP 协议就是基于 JSON-RPC，可以非常轻松的实现客户端远程调用服务端的函数。MCP 中所有通信都遵循 JSON-RPC 2.0，2.0 相比 1.0 做了各个方面的优化：

Requests 请求（双向可发）

```json
{
  "jsonrpc": "2.0",
  "id": 1,
  "method": "callTool",
  "params": { ... }
}
```

- 必须包含 `id`（唯一）
- 用于发起双向交互

Responses 响应（用于回应请求）

```json
{
  "jsonrpc": "2.0",
  "id": 1,
  "result": { ... }
}
```

```json
{
  "jsonrpc": "2.0",
  "id": 1,
  "error": { "code": 1001, "message": "Tool not found" }
}
```

- 必须响应对应请求 ID
- 只能出现 result 或 error（二选一）

Notifications 通知（单向）

```json
{
  "jsonrpc": "2.0",
  "method": "logEvent",
  "params": { ... }
}
```

- **不能包含 ID**，因此接收方不能响应
- 用于单向通知，如：日志、状态变化等



### 通信机制

官方目前提供了两种标准的通信方式：

**1. stdio 流通信**

stdio（标准输入输出）传输方式允许通过标准输入（stdin）和标准输出（stdout）进行通信。

- 通常用于 MCP Server 是一个本地 Node.js、Python、Rust 等脚本
- MCP Client 以子进程方式启动 Server，并通过 stdin/stdout 与它通信（无需 HTTP）

```js
node xxx.js
```

```js
// 现在是在 MCP Client 里面
import { spawn } from "child_process";
spawn(...); 
```



以下场景推荐使用 stdio：

1. 构建命令行工具时
2. 实现本地系统集成时（如与本机软件交互）
3. 需要进程之间的简单通信时（如子进程）



**2. Streamable HTTP**

Streamable HTTP 是一种传输机制，这种方式是基于 HTTP 协议：

- 请求 - 使用 POST 请求将客户端消息发送给服务器
- 响应 - 支持两种形式：
  - 普通 JSON 响应：适用于简单返回结果
  - SSE 流式响应：用于需要多次分段推送内容的场景（如生成流）

这是 MCP 协议推荐的新一代通信方式，用 HTTP 双向传递消息，兼顾兼容性与流式能力

以下场景推荐使用 Streamable HTTP：

- 开发基于 Web 的集成服务时（如 SaaS 工具、网页应用）
- 需要使用 HTTP 实现客户端与服务器之间通信时
- 需要维护状态会话时（stateful session，比如对话上下文）
- 需要支持多个客户端并发连接时
- 需要支持可恢复连接（resumable connection）时



### MCP Server

目前官方组织推出了一些 [MCP Server](https://github.com/modelcontextprotocol/servers)

除了官方以外，也有一些第三方的 MCP Server 的平台，例如：

1. [MCP.So](https://mcp.so/)
2. [Awesome MCP Servers](https://mcpservers.org/)

>[!tip]
>
>不过目前第三方平台的 MCP Server 的质量参差不齐，推荐优先使用官方推出的 MCP Server。

**课堂练习**

实现天气查询的 MCP Server



### MCP Client

**课堂练习**

实现MCP Client



## 依赖审计

前置知识：

1. npm-registry-fetch
2. zod
3. CVE编号与 CVSS 分数



**CVE 编号（Common Vulnerabilities and Exposures）**

- 是全球通用的安全漏洞编号系统。例如：`CVE-2024-12345`
- 每一个已知的安全漏洞（比如某个 npm 包的漏洞）都会被分配一个唯一的 CVE 编号，便于在安全数据库中查找。
- 你可以在 [这里](https://github.com/advisories) 看到安全漏洞的描述以及 CVE 编号。



**CVSS 分数（Common Vulnerability Scoring System）**

- 是对漏洞“危险程度”的一个评分系统，分数从 0 到 10：
  - 0.0：无风险
  - 4.0~6.9：中等
  - 7.0~8.9：高危
  - 9.0~10.0：严重
- 开发者可以用这个分数判断漏洞是否值得立即修复。

---

-EOF-
